buildscript {
  repositories {
    jcenter()
    mavenCentral()
    mavenLocal()
  }
  dependencies {
    classpath "io.spring.gradle:dependency-management-plugin:0.5.4.RELEASE"
  }

  apply from: file('gradle/buildscript.gradle'), to: buildscript
}

allprojects {
  repositories {
    mavenCentral()
    mavenLocal()
    maven {
      url 'https://repository.apache.org/content/repositories/snapshots/'
    }
  }
}

//apply from: file('gradle/convention.gradle')
//apply from: file('gradle/maven.gradle')
apply from: file('gradle/check.gradle')
//apply from: file('gradle/license.gradle')
//apply from: file('gradle/release.gradle')
apply from: file("gradle/dependency-versions.gradle")


subprojects { project ->
  apply plugin: "java"
  apply plugin: "io.spring.dependency-management"
  sourceCompatibility = 1.8
  status = rootProject.status
  dependencyManagement {
    imports {
      mavenBom 'com.amazonaws:aws-java-sdk-bom:1.10.47'
    }
  }
}

project(':fdbench-core') {
  dependencies {
    compile project(':fdbench-api')
    compile project(':fdbench-utils')
    compile "com.google.guava:guava:$guavaVersion"
    compile "com.googlecode.objectify:objectify:$objectifyVersion"
    compile "com.typesafe:config:$typesafeConfigVersion"
    compile "commons-cli:commons-cli:$commonsCLIVersion"
    compile "org.apache.commons:commons-lang3:$commonsLangVersion"
    compile "com.google.code.gson:gson:$gsonVersion"
    compile "com.google.guava:guava:$guavaVersion"
    compile "org.apache.samza:samza-api:$samzaVersion"
    compile "org.apache.samza:samza-core_2.10:$samzaVersion"
    compile "ch.qos.logback:logback-classic:$logbackVersion"
    compile "com.amazonaws:aws-java-sdk-dynamodb"
    testCompile "junit:junit:$junitVersion"
  }
}

project(':fdbench-samza') {
  dependencies {
    compile project(':fdbench-core')
    compile "com.google.guava:guava:$guavaVersion"
    compile "com.googlecode.objectify:objectify:$objectifyVersion"
    compile "com.typesafe:config:$typesafeConfigVersion"
    compile "commons-cli:commons-cli:$commonsCLIVersion"
    compile "org.apache.commons:commons-lang3:$commonsLangVersion"
    compile "org.hdrhistogram:HdrHistogram:$hdrHistogramVersion"
    compile "com.google.code.gson:gson:$gsonVersion"
    compile "com.google.guava:guava:$guavaVersion"
    compile "org.apache.samza:samza-api:$samzaVersion"
    compile "org.apache.samza:samza-core_2.10:$samzaVersion"
    compile "org.apache.samza:samza-yarn_2.10:$samzaVersion"
    compile "org.apache.samza:samza-kafka_2.10:$samzaVersion"
    compile "org.apache.samza:samza-kv-rocksdb_2.10:$samzaVersion"
    compile "org.apache.samza:samza-autoscaling_2.10:$samzaVersion"
    compile "org.apache.samza:samza-hdfs_2.10:$samzaVersion"
    compile "org.apache.samza:samza-kv_2.10:$samzaVersion"
    compile "ch.qos.logback:logback-classic:$logbackVersion"
    compile "com.amazonaws:aws-java-sdk-dynamodb"
    testCompile "junit:junit:$junitVersion"
  }
}

project(':fdbench-automation') {
  dependencies {
    compile "com.beust:jcommander:$jCommanderVersion"
    compile "com.typesafe:config:$typesafeConfigVersion"
    compile "org.freemarker:freemarker:$freemakerVersion"
    compile "org.hdrhistogram:HdrHistogram:$hdrHistogramVersion"
    compile "com.google.guava:guava:$guavaVersion"
    testCompile "junit:junit:$junitVersion"
  }
}

project(':fdbench-api') {
  dependencies {
    compile "org.apache.samza:samza-api:$samzaVersion"
    compile "org.apache.samza:samza-core_2.10:$samzaVersion"
    compile "com.typesafe:config:$typesafeConfigVersion"
    compile "org.hdrhistogram:HdrHistogram:$hdrHistogramVersion"
  }
}

project(':fdbench-utils') {
  dependencies {
    compile project(':fdbench-api')
    compile 'com.amazonaws:aws-java-sdk-dynamodb'
    compile "ch.qos.logback:logback-classic:$logbackVersion"
  }
}

project(':fdbench-datagen') {
  dependencies {
    compile "com.google.guava:guava:$guavaVersion"
    compile "com.googlecode.objectify:objectify:$objectifyVersion"
    compile "com.typesafe:config:$typesafeConfigVersion"
    compile "com.beust:jcommander:$jCommanderVersion"
    compile "org.apache.commons:commons-lang3:$commonsLangVersion"
    compile "com.google.code.gson:gson:$gsonVersion"
    compile "com.google.guava:guava:$guavaVersion"
    compile "ch.qos.logback:logback-classic:$logbackVersion"
    compile "com.amazonaws:aws-java-sdk-dynamodb"
    compile "org.freemarker:freemarker:$freemakerVersion"
    compile "org.apache.kafka:kafka-clients:$kafkaVersion"
    compile "org.apache.kafka:kafka_2.11:$kafkaVersion"
    testCompile "junit:junit:$junitVersion"
  }
}

project(':fdbench-tools') {
  dependencies {
    compile "com.google.guava:guava:$guavaVersion"
    compile "com.googlecode.objectify:objectify:$objectifyVersion"
    compile "com.typesafe:config:$typesafeConfigVersion"
    compile "com.beust:jcommander:$jCommanderVersion"
    compile "org.apache.commons:commons-lang3:$commonsLangVersion"
    compile "com.google.code.gson:gson:$gsonVersion"
    compile "com.google.guava:guava:$guavaVersion"
    compile "ch.qos.logback:logback-classic:$logbackVersion"
    compile "com.amazonaws:aws-java-sdk-dynamodb"
    compile "org.freemarker:freemarker:$freemakerVersion"
    testCompile "junit:junit:$junitVersion"
  }

  task plotResults(type: FDBenchThroughputResultPlotterTask) {
    tableName "fdbench-producer-metrics-0802"
    benchmarkName "fdbench-producer-throughput-13"
    outputDir new File(project.buildDir.absolutePath + File.separator + "results")
  }
}

project(':fdbench-kafka') {
  dependencies {
    compile project(':fdbench-core')
    compile "ch.qos.logback:logback-classic:$logbackVersion"
    compile "org.apache.kafka:kafka-clients:$kafkaVersion"
    compile "org.apache.kafka:kafka_2.11:$kafkaVersion"
    compile "org.apache.kafka:kafka-tools:$kafkaVersion"
    compile "org.scala-lang:scala-library:$scalaLibVersion"
    compile "com.101tec:zkclient:$zkClientVersion"
    compile "com.googlecode.objectify:objectify:$objectifyVersion"
    compile "com.typesafe:config:$typesafeConfigVersion"
    compile "commons-cli:commons-cli:$commonsCLIVersion"
    compile "org.apache.commons:commons-lang3:$commonsLangVersion"
    compile "org.apache.commons:commons-math3:$commonsMath3Version"
    compile "org.hdrhistogram:HdrHistogram:$hdrHistogramVersion"
    compile "com.google.code.gson:gson:$gsonVersion"
    compile "com.google.guava:guava:$guavaVersion"
    compile "org.apache.samza:samza-api:$samzaVersion"
    compile "org.apache.samza:samza-core_2.10:$samzaVersion"
    testCompile "junit:junit:$junitVersion"
  }
}

project(':fdbench-yarn') {
  dependencies {
    compile project(':fdbench-core')
    compile project(':fdbench-kafka')
    compile "com.google.guava:guava:$guavaVersion"
    compile "org.apache.hadoop:hadoop-yarn-api:$yarnVersion"
    compile "org.apache.hadoop:hadoop-yarn-common:$yarnVersion"
    compile "org.apache.hadoop:hadoop-yarn-client:$yarnVersion"
    compile "org.apache.hadoop:hadoop-common:$yarnVersion"
    compile "org.apache.hadoop:hadoop-hdfs:$yarnVersion"
  }

  task tar(type: Tar) {
    compression = Compression.GZIP
    classifier = 'dist'

    into('bin') {
      from 'src/main/bash'
      fileMode = 0755
    }

    into('lib') {
      from configurations.runtime
    }

    into('lib') {
      from jar
    }
  }

  task testConsumerThroughput(type: FDBenchRunnerTask, dependsOn: [tar]) {
    benchmark "fdbench-consumer-throughput"
    benchmarkDesc "Measuring consumer throughput"
    configFile "consumer-throughput.conf"
    topic "bench-topic-1"
    deleteTopic false
    reuseTopic true
    metricsTable 'fdbench-consumer-metrics-ec2-1005-1'
    partitions 1
    maxRecords 100000000
  }

  task testProducerThroughput(type: FDBenchRunnerTask, dependsOn: [tar]) {
    benchmark "fdbench-producer-throughput"
    benchmarkDesc "Measuring producer throughput"
    configFile "producer-throughput.conf"
    topic "bench-topic-5"
    deleteTopic false
    reuseTopic false
    metricsTable 'fdbench-producer-metrics-ec2-1005-p1-c3-1'
    partitions 1
    maxThroughput 1000000
    maxRecords 100000000
    parallelism 3
  }

  task killYarnApp(type: JavaExec) {
    classpath = sourceSets.main.runtimeClasspath + files('/Users/mpathira/PhD/Code/vagrant-hadoop/resources/hadoop')
    main = 'org.apache.hadoop.yarn.client.cli.ApplicationCLI'

    environment 'HADOOP_CONF_DIR', '/Users/mpathira/PhD/Code/vagrant-hadoop/resources/hadoop'

    if (project.hasProperty('app')) {
      println("Killing application: " + project.getProperties().get('app'))
      args(['application', '-kill', project.getProperties().get('app')])
    }
  }
}

class JavaExecWithEnv extends JavaExec {
  String getEnv(String name, String defaultValue) {
    return System.getenv(name) != null ? System.getenv(name) : defaultValue
  }

  boolean isENVVarExists(String varName) {
    String varValue = System.getenv(varName)
    if (varValue != null && !varValue.empty) {
      return true
    }

    return false
  }

  int getNextNumberInSequence(String sequence) {
    def homeDir = System.getProperty('user.home')
    def fdbenchDir = homeDir + File.separator + '.fdbench'
    if(!new File(fdbenchDir).exists()) {
      if(!new File(fdbenchDir).mkdirs()){
        throw new RuntimeException("Couldn't create FDBench config directory: " + fdbenchDir)
      }
    }

    if(!new File(fdbenchDir).isDirectory()) {
      throw new RuntimeException(fdbenchDir + " is not a directory!")
    }

    def sequenceFile = new File(fdbenchDir + File.separator + sequence)
    if(!sequenceFile.exists()) {
      sequenceFile.write 1.toString()
      return 1
    } else {
      def v = Integer.valueOf(sequenceFile.text) + 1
      sequenceFile.write v.toString()
      return v
    }
  }
}

class JavaExecWithAWS extends JavaExecWithEnv {
  String getAWSAccessKey() {
    return System.getenv('AWS_ACCESS_KEY_ID') == null || System.getenv('AWS_ACCESS_KEY_ID').empty ?
        System.getenv('AWS_ACCESS_KEY') : System.getenv('AWS_ACCESS_KEY_ID')
  }

  String getAWSSecretAccessKey() {
    return System.getenv('AWS_SECRET_ACCESS_KEY') == null || System.getenv('AWS_SECRET_ACCESS_KEY').empty ?
        System.getenv('AWS_SECRET_KEY') : System.getenv('AWS_SECRET_ACCESS_KEY')
  }

  void verifyAWSCredentials() {
    if (!(isENVVarExists('AWS_ACCESS_KEY_ID') && isENVVarExists('AWS_SECRET_ACCESS_KEY') ||
        isENVVarExists('AWS_ACCESS_KEY') && isENVVarExists('AWS_SECRET_KEY'))) {
      throw new RuntimeException('Cannot find AWS credentials in environment.')
    }
  }
}

class JavaExecWithKafka extends JavaExecWithAWS {
  void verifyKafkaConfig() {
    if(!isENVVarExists('ZK_CONNECTION_STR')) {
      throw new RuntimeException('Cannot find environment variable ZK_CONNECTION_STR')
    }
    if(!isENVVarExists('KAFKA_BROKERS')) {
      throw new RuntimeException('Cannot find environment variable KAFKA_BROKERS')
    }
  }
}

class FDBenchThroughputResultPlotterTask extends JavaExecWithAWS {
  String tableName
  String benchmarkName
  File outputDir

  FDBenchThroughputResultPlotterTask() {
    classpath = project.sourceSets.main.runtimeClasspath
    main = 'org.pathirage.fdbench.tools.ThroughputResultPlotter'
  }

  @Override
  void exec() {
    verifyAWSCredentials()
    if(!outputDir.exists()) {
      if(!outputDir.mkdirs()) {
        throw new RuntimeException("Couldn't create output directory " + outputDir.getAbsolutePath())
      }
    }

    this.setArgs(['-t', tableName, '-b', benchmarkName, '-i', getAWSAccessKey(), '-s', getAWSSecretAccessKey(),
    '-o', outputDir.getAbsolutePath()])
    super.exec()
  }
}

class FDBenchRunnerTask extends JavaExecWithKafka {
  String benchmark
  String benchmarkDesc
  String configFile
  String topic = "fdbench-topic"
  boolean deleteTopic = true
  boolean reuseTopic = false
  int partitions = 1
  int replicationFactor = 1
  int parallelism = 1
  int maxRecords = 50000000
  int maxThroughput = 50000
  String metricsTable = 'fdbench-metrics-0716'
  File deployDir = new File(project.buildDir.absolutePath + '/deploy')
  File configDir = new File(project.projectDir.absolutePath + '/src/main/resources')

  FDBenchRunnerTask() {
    classpath = project.sourceSets.main.runtimeClasspath + project.files(getEnv('HADOOP_CONF_DIR', '/usr/local/hadoop'))
    main = 'org.pathirage.fdbench.BenchRunner'
  }

  String getEnv(String name, String defaultValue) {
    return System.getenv(name) != null ? System.getenv(name) : defaultValue
  }

  @Override
  void exec() {
    verifyAWSCredentials()
    verifyKafkaConfig()
    if (!deployDir.exists()) {
      if (!deployDir.mkdirs()) {
        throw new RuntimeException("Couldn't create configuration directory " + deployDir.getAbsolutePath())
      }
    }
    project.copy {
      from configDir
      include configFile
      into deployDir
      expand([
          benchmark: benchmark + '-' + getNextNumberInSequence(),
          benchmarkDescription: benchmarkDesc,
          parallelism: parallelism,
          zkConnect: System.getenv('ZK_CONNECTION_STR'),
          kafkaBrokers: System.getenv('KAFKA_BROKERS'),
          topic: topic,
          deleteTopic: deleteTopic,
          reuseTopic: reuseTopic,
          partitions: partitions,
          replicationFactor: replicationFactor,
          maxRecords: maxRecords,
          maxThroughput: maxThroughput,
          awsAccessKey: getAWSAccessKey(),
          awsSecretAccessKey: getAWSSecretAccessKey(),
          metricsTable: metricsTable,
          projectDir: project.projectDir
      ])
    }
    this.setArgs(['-c', deployDir.absolutePath + '/' + configFile])
    super.exec()
  }
}