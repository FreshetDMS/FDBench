buildscript {
  repositories {
    jcenter()
    mavenCentral()
    mavenLocal()
  }
  dependencies {
    classpath "io.spring.gradle:dependency-management-plugin:0.5.4.RELEASE"
  }

  apply from: file('gradle/buildscript.gradle'), to: buildscript
}

allprojects {
  repositories {
    mavenCentral()
    mavenLocal()
    maven {
      url 'https://repository.apache.org/content/repositories/snapshots/'
    }
  }
}

//apply from: file('gradle/convention.gradle')
//apply from: file('gradle/maven.gradle')
apply from: file('gradle/check.gradle')
//apply from: file('gradle/license.gradle')
//apply from: file('gradle/release.gradle')
apply from: file("gradle/dependency-versions.gradle")


subprojects { project ->
  apply plugin: "java"
  apply plugin: "io.spring.dependency-management"
  sourceCompatibility = 1.8
  status = rootProject.status
  dependencyManagement {
    imports {
      mavenBom 'com.amazonaws:aws-java-sdk-bom:1.10.47'
    }
  }
}

project(':fdbench-core') {


  dependencies {
    compile "com.google.guava:guava:$guavaVersion"
    compile 'com.googlecode.objectify:objectify:5.1.12'
    compile 'com.typesafe:config:1.3.0'
    compile 'commons-cli:commons-cli:1.3.1'
    compile 'org.apache.commons:commons-lang3:3.4'
    compile 'org.hdrhistogram:HdrHistogram:2.1.8'
    compile 'com.google.code.gson:gson:2.7'
    compile 'com.google.guava:guava:19.0'
    compile 'org.apache.samza:samza-api:0.10.0'
    compile 'org.apache.samza:samza-core_2.10:0.10.0'
    compile 'ch.qos.logback:logback-classic:1.0.13'
    compile 'com.amazonaws:aws-java-sdk-dynamodb'
    testCompile 'junit:junit:4.12'
  }
}

project(':fdbench-kafka') {
  dependencies {
    compile project(':fdbench-core')
    compile 'ch.qos.logback:logback-classic:1.0.13'
    compile 'org.apache.kafka:kafka-clients:0.9.0.1'
    compile 'org.apache.kafka:kafka_2.11:0.9.0.1'
    compile 'org.apache.kafka:kafka-tools:0.9.0.1'
    compile 'org.scala-lang:scala-library:2.11.8'
    compile 'com.101tec:zkclient:0.8'
    compile 'com.googlecode.objectify:objectify:5.1.12'
    compile 'com.typesafe:config:1.3.0'
    compile 'commons-cli:commons-cli:1.3.1'
    compile 'org.apache.commons:commons-lang3:3.4'
    compile 'org.hdrhistogram:HdrHistogram:2.1.8'
    compile 'com.google.code.gson:gson:2.7'
    compile 'com.google.guava:guava:19.0'
    compile 'org.apache.samza:samza-api:0.10.0'
    compile 'org.apache.samza:samza-core_2.10:0.10.0'
    testCompile 'junit:junit:4.12'
  }
}

project(':fdbench-yarn') {
  dependencies {
    compile project(':fdbench-core')
    compile project(':fdbench-kafka')
    compile "com.google.guava:guava:$guavaVersion"
    compile 'org.apache.hadoop:hadoop-yarn-api:2.7.1'
    compile 'org.apache.hadoop:hadoop-yarn-common:2.7.1'
    compile 'org.apache.hadoop:hadoop-yarn-client:2.7.1'
    compile 'org.apache.hadoop:hadoop-common:2.7.1'
    compile 'org.apache.hadoop:hadoop-hdfs:2.7.1'
  }

  task tar(type: Tar) {
    compression = Compression.GZIP
    classifier = 'dist'

    into('bin') {
      from 'src/main/bash'
      fileMode = 0755
    }

    into('lib') {
      from configurations.runtime
    }

    into('lib') {
      from jar
    }
  }

  task testProducerThroughput(type: FDBenchRunnerTask, dependsOn: [tar]) {
    benchmark "fdbench-producer-throughput"
    benchmarkDesc "Measuring producer throughput"
    configFile "producer-throughput.conf"
    topic "producethroughputbench" + new Random(System.currentTimeMillis()).nextInt(1000)
  }

  task killYarnApp(type: JavaExec) {
    classpath = sourceSets.main.runtimeClasspath + files('/Users/mpathira/PhD/Code/vagrant-hadoop/resources/hadoop')
    main = 'org.apache.hadoop.yarn.client.cli.ApplicationCLI'

    environment 'HADOOP_CONF_DIR', '/Users/mpathira/PhD/Code/vagrant-hadoop/resources/hadoop'

    if (project.hasProperty('app')) {
      println("Killing application: " + project.getProperties().get('app'))
      args(['application', '-kill', project.getProperties().get('app')])
    }
  }
}

class FDBenchRunnerTask extends JavaExec {
  String benchmark
  String benchmarkDesc
  String configFile
  String topic = "fdbench-topic"
  int partitions = 1
  int replicationFactor = 1
  int parallelism = 1
  int maxRecords = 500000
  int maxThroughput = 10000
  String metricsTable = 'fdbench-metrics'
  File deployDir = new File(project.buildDir.absolutePath + '/deploy')
  File configDir = new File(project.projectDir.absolutePath + '/src/main/resources')

  FDBenchRunnerTask() {
    classpath = project.sourceSets.main.runtimeClasspath + project.files(System.getenv('HADOOP_CONF_DIR'))
    main = 'org.pathirage.fdbench.BenchRunner'
  }

  @Override
  void exec() {
    verifyAWSCredentials()
    verifyKafkaConfig()
    if (!deployDir.exists()) {
      if (!deployDir.mkdirs()) {
        throw new RuntimeException("Couldn't create configuration directory " + deployDir.getAbsolutePath())
      }
    }
    project.copy {
      from configDir
      include configFile
      into deployDir
      expand([
          benchmark: benchmark,
          benchmarkDescription: benchmarkDesc,
          parallelism: parallelism,
          zkConnect: System.getenv('ZK_CONNECTION_STR'),
          kafkaBrokers: System.getenv('KAFKA_BROKERS'),
          topic: topic,
          partitions: partitions,
          replicationFactor: replicationFactor,
          maxRecords: maxRecords,
          maxThroughput: maxThroughput,
          awsAccessKey: getAWSAccessKey(),
          awsSecretAccessKey: getAWSSecretAccessKey(),
          metricsTable: metricsTable,
          projectDir: project.projectDir
      ])
    }
    this.setArgs(['-c', deployDir.absolutePath + '/' + configFile])
    super.exec()
  }

  void verifyKafkaConfig() {
    if(!isENVVarExists('ZK_CONNECTION_STR')) {
      throw new RuntimeException('Cannot find environment variable ZK_CONNECTION_STR')
    }
    if(!isENVVarExists('KAFKA_BROKERS')) {
      throw new RuntimeException('Cannot find environment variable KAFKA_BROKERS')
    }
  }


  String getAWSAccessKey() {
    return System.getenv('AWS_ACCESS_KEY_ID') == null || System.getenv('AWS_ACCESS_KEY_ID').empty ?
        System.getenv('AWS_ACCESS_KEY') : System.getenv('AWS_ACCESS_KEY_ID')
  }

  String getAWSSecretAccessKey() {
    return System.getenv('AWS_SECRET_ACCESS_KEY') == null || System.getenv('AWS_SECRET_ACCESS_KEY').empty ?
        System.getenv('AWS_SECRET_KEY') : System.getenv('AWS_SECRET_ACCESS_KEY')
  }

  void verifyAWSCredentials() {
    if (!(isENVVarExists('AWS_ACCESS_KEY_ID') && isENVVarExists('AWS_SECRET_ACCESS_KEY') ||
        isENVVarExists('AWS_ACCESS_KEY') && isENVVarExists('AWS_SECRET_KEY'))) {
      throw new RuntimeException('Cannot find AWS credentials in environment.')
    }
  }

  boolean isENVVarExists(String varName) {
    String varValue = System.getenv(varName)
    if (varValue != null && !varValue.empty) {
      return true
    }

    return false
  }
}